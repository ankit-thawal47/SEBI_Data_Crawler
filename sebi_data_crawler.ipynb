{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp SebiDataCrawler.end_to_end_crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEBI DATA CRAWLER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any Dependencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "import logging\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def setup_logger(log_file):\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Set the timezone to Indian Standard Time\n",
    "    ist = pytz.timezone('Asia/Kolkata')\n",
    "\n",
    "    # Create a formatter that includes the timestamp in IST\n",
    "    class ISTFormatter(logging.Formatter):\n",
    "        def converter(self, timestamp):\n",
    "            dt = datetime.fromtimestamp(timestamp)\n",
    "            dt = pytz.utc.localize(dt)\n",
    "            return dt.astimezone(ist)\n",
    "\n",
    "    formatter = ISTFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Create a file handler\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    fh.setFormatter(formatter)\n",
    "\n",
    "    # Add the file handler to the logger\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # Create a console handler\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # Add the console handler to the logger\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    return logger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "#|eval: true\n",
    "#|code-fold: True\n",
    "\n",
    "import os\n",
    "\n",
    "'''\n",
    "legal_menu_and_sub_menu is dict to store the urls to access \"Historical Data\" from the legal menu.\n",
    "It's hard to collect these links, hence need to hardcode.\n",
    "'''\n",
    "legal_menu_and_sub_menu = {\n",
    "            \"acts\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListingLegal=yes&sid=1&ssid=1&smid=0\",\n",
    "            \"rules\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListingLegal=yes&sid=1&ssid=2&smid=0\",\n",
    "            \"regulations\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListingLegal=yes&sid=1&ssid=3&smid=0\",\n",
    "            \"general_orders\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=4&smid=0\",\n",
    "            \"guidelines\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=5&smid=0\",\n",
    "            \"master_circulars\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=6&smid=0\",\n",
    "            \"circulars\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=7&smid=0\",\n",
    "            # Doubt ? => How is Circulars are Circular_archive are different?\n",
    "            \"circulars_archive\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListingCirArchive=yes&sid=1&ssid=7&smid=0\",\n",
    "            \"gazette_notification\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=82&smid=0\",\n",
    "            \"online_application_portal\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=91&smid=0\",\n",
    "            \"guidance_notes\" : \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=1&ssid=85&smid=0\",\n",
    "        }\n",
    "\n",
    "\n",
    "# '''DB CONSTANTS'''\n",
    "import os\n",
    "\n",
    "datanase_name = \"crawled_data.sqlite3\"\n",
    "database_path = os.path.join(\"..\", \"databases\", datanase_name)\n",
    "DB_NAME = database_path\n",
    "\n",
    "TABLE_SEBI_RECORDS = \"sebi_records\"\n",
    "# '''Paths of Folders and CSV files''' \n",
    "home_page_url = \"https://www.sebi.gov.in\"\n",
    "\n",
    "regChat_folder = \"\"\n",
    "extraction_folder_name = \"SEBI_Extracted_Data\"\n",
    "\n",
    "base_folder_path = os.path.join(regChat_folder,extraction_folder_name)\n",
    "\n",
    "SEBI_data_extraction_base_folder = os.path.join(regChat_folder,extraction_folder_name)\n",
    "\n",
    "urls_of_sebi_menu_csv_path = os.path.join(regChat_folder,\"urls_of_menus_of_sebi.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Menus and Folder Creator\n",
    "\n",
    "1. Use menu_of_sebi.js file from SEBI to collect initial set of links. => save them into csv file\n",
    "2. Using these menus and submenus collected to create hierachy of folders\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "#|eval: true\n",
    "#|code-fold: True\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class SEBIMenuCollector:\n",
    "    \n",
    "        def dictify(self, ul_element):\n",
    "            '''\n",
    "            Recursively converts nested HTML <ul> elements into a multi-level dictionary.\n",
    "\n",
    "            Args:\n",
    "                ul_element (bs4.element.Tag): BeautifulSoup Tag object representing the <ul> element.\n",
    "\n",
    "            Returns:\n",
    "                dict: A multi-level dictionary representing the menu structure.\n",
    "\n",
    "            Use? => This method is used to create a nested folder structure and \n",
    "                    maintain hierarchy for storing menus and submenus.\n",
    "\n",
    "            '''\n",
    "            result = {}\n",
    "            # Iterate over each <li> element within the <ul> element\n",
    "            for li_element in ul_element.find_all(\"li\", recursive=False):\n",
    "                # Extract the text content of the <li> element to use as the dictionary key\n",
    "                key = next(li_element.stripped_strings)\n",
    "                url = \"\"\n",
    "                # Check if the <li> element contains an <a> tag (link)\n",
    "                a_tags = li_element.find_all('a')\n",
    "                if len(a_tags) == 1:\n",
    "                    # If only one <a> tag exists, extract the URL\n",
    "                    url = a_tags[0]['href']\n",
    "                    \n",
    "                # Check if the <li> element contains a nested <ul> element (submenu)\n",
    "                ul_sub_element = li_element.find(\"ul\")\n",
    "                if ul_sub_element:\n",
    "                    # Recursively call dictify() to handle nested menus\n",
    "                    result[key] = self.dictify(ul_sub_element)\n",
    "                else:\n",
    "                    # If no submenu exists, check and normalize the URL\n",
    "                    if not url.startswith(\"http\"):\n",
    "                        base_url = \"https://www.sebi.gov.in\"\n",
    "                        # Ensure URL is absolute by joining it with the base URL\n",
    "                        joined_url = urljoin(base_url, url)\n",
    "                        url = joined_url\n",
    "                    # Assign the URL to the dictionary key\n",
    "                    result[key] = url\n",
    "            return result\n",
    "\n",
    "        \n",
    "        def download_menus_js(self):\n",
    "            \"\"\"\n",
    "            Downloads SEBI menus JavaScript file and extracts its content.\n",
    "\n",
    "            This method fetches the JavaScript file containing SEBI website menus\n",
    "            from the URL 'https://www.sebi.gov.in/js/menu.js'. It then processes\n",
    "            the JavaScript content to extract the menu structure by removing \n",
    "            JavaScript-specific syntax. The resulting HTML content is wrapped in \n",
    "            <html> tags to ensure proper structure.\n",
    "\n",
    "            Returns:\n",
    "                str: The HTML content of the SEBI menus extracted from the JavaScript file.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Define the URL of the JavaScript file containing SEBI menus\n",
    "            menu_js_url = \"https://www.sebi.gov.in/js/menu.js\"\n",
    "\n",
    "            # Fetch the content of the JavaScript file\n",
    "            response = requests.get(menu_js_url)\n",
    "            html_content = response.text\n",
    "\n",
    "            # Process JavaScript content to extract menu structure\n",
    "            html_content = html_content.replace('document.write(\"', \"\")\n",
    "            html_content = html_content.replace('\");', '')\n",
    "            html_content = \"<html>\" + html_content + \"</html>\"\n",
    "\n",
    "            return html_content\n",
    "\n",
    "    \n",
    "        def collect_menu_links(self):\n",
    "            \n",
    "            '''\n",
    "            Collects menu links from the SEBI website and stores them in a CSV file.\n",
    "\n",
    "            This method retrieves the menu structure from the SEBI website, parses it,\n",
    "            and organizes it into a DataFrame. It then normalizes and prepares the data\n",
    "            for storage in a CSV file. The function also ensures that historical data \n",
    "            links are included and properly formatted. Finally, it appends the data to \n",
    "            an existing CSV file or creates a new one if it doesn't exist.\n",
    "\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            # OLD => saving menu of sebis into csv file\n",
    "            # NEW => We will maintain CSV for Menu collection\n",
    "            if(os.path.exists(urls_of_sebi_menu_csv_path)):\n",
    "                return\n",
    "            # OLD => manually downladed fileissues? \n",
    "            # NEW => instead of downloaded file, download the file from the server\n",
    "            \n",
    "            # OLD => reads a manually saved menus_of_sebi.html file and stores content into html_content\n",
    "            # NEW => write a method that will get the data using requests.get and store into\n",
    "            html_content = self.download_menus_js()\n",
    "            \n",
    "            # Parse the HTML\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "            # Get the main list\n",
    "            ul = soup.ul\n",
    "            result = self.dictify(ul)\n",
    "\n",
    "            # Example data dictionary\n",
    "            data_dict = result\n",
    "\n",
    "            # Initialize empty lists for each column\n",
    "            menu_list, submenu_list, description_list, url_list = [], [], [], []\n",
    "\n",
    "            # Iterate over the data dictionary and append values to the lists\n",
    "            for menu, submenu_dict in data_dict.items():\n",
    "                for submenu, details in submenu_dict.items():\n",
    "                    if isinstance(details, dict):\n",
    "                        for description, url in details.items():\n",
    "                            menu_list.append(menu.lower())\n",
    "                            submenu_list.append(submenu.lower())\n",
    "                            description_list.append(description.lower())\n",
    "                            url_list.append(url)\n",
    "                    else:\n",
    "                        menu_list.append(menu.lower())\n",
    "                        submenu_list.append(submenu.lower())\n",
    "                        description_list.append(\"\")\n",
    "                        url_list.append(details)\n",
    "\n",
    "            # Create DataFrame from lists\n",
    "            df = pd.DataFrame({\n",
    "                'menu': menu_list,\n",
    "                'submenu': submenu_list,\n",
    "                'subsubmenu': description_list,\n",
    "                'url': url_list\n",
    "            })\n",
    "            \n",
    "            row_count = len(df)\n",
    "            for i in range(row_count):\n",
    "                row = df.iloc[i]\n",
    "                menu = row['menu']\n",
    "                submenu = row['submenu']\n",
    "                subsubmenu = row['subsubmenu']\n",
    "                print(menu, submenu, subsubmenu)\n",
    "                df.at[i, 'menu'] = menu.replace(\" \",\"_\")\n",
    "                df.at[i, 'submenu'] = submenu.replace(\" \",\"_\")\n",
    "                df.at[i, 'subsubmenu'] = subsubmenu.replace(\" \",\"_\")\n",
    "            print(\"replaced <space> with <_>\")\n",
    "\n",
    "            '''df2 maintains historical_data links'''\n",
    "            df2 = pd.DataFrame(\n",
    "                {\n",
    "                    'menu': [\"legal\"] * len(legal_menu_and_sub_menu),\n",
    "                    'submenu': list(legal_menu_and_sub_menu.keys()),\n",
    "                    'subsubmenu': [\"historical_data\"] * len(legal_menu_and_sub_menu),\n",
    "                    'url': list(legal_menu_and_sub_menu.values())\n",
    "                }\n",
    "            )\n",
    "\n",
    "            final_df = pd.concat([df, df2], axis=0)\n",
    "            \n",
    "            # OLD => saved the data to csv file\n",
    "            # NEW => No change its better to save the menus_data in csv file only\n",
    "            final_df.to_csv(urls_of_sebi_menu_csv_path, mode='a')\n",
    "            \n",
    "            \n",
    "        def create_folder_hierarchy(self):\n",
    "            '''\n",
    "            Creates a folder hierarchy based on the SEBI menu structure.\n",
    "\n",
    "            This method reads the SEBI menu structure from a CSV file, which contains \n",
    "            menu and submenu information. It then creates a folder hierarchy based on \n",
    "            this structure in the specified base folder path. Each menu and submenu \n",
    "            name is normalized by replacing spaces with underscores and converting to \n",
    "            lowercase. If the base folder or any menu/submenu folder doesn't exist, \n",
    "            it creates them.\n",
    "            '''\n",
    "            try:\n",
    "                df = pd.read_csv(urls_of_sebi_menu_csv_path)\n",
    "\n",
    "                # Base folder to store SEBI Extracted Data\n",
    "                print(\"Creating Folder Hierarchy for : \",base_folder_path)\n",
    "                if not os.path.exists(base_folder_path):\n",
    "                    # Create the base folder if it doesn't exist\n",
    "                    os.makedirs(base_folder_path)\n",
    "\n",
    "                for _,row in df.iterrows():\n",
    "                    menu = row['menu']\n",
    "                    menu = menu.replace(\" \",\"_\")\n",
    "                    menu = menu.lower()\n",
    "                    print(menu)\n",
    "                    menu_folder = os.path.join(base_folder_path,menu)\n",
    "                    # Create the base folder if it doesn't exist\n",
    "                    if not os.path.exists(menu_folder):\n",
    "                        os.makedirs(menu_folder)\n",
    "                    sub_menu = row['submenu']\n",
    "                    sub_menu = sub_menu.lower()\n",
    "                    sub_menu = sub_menu.replace(\" \",\"_\")\n",
    "                    sub_menu_folder = os.path.join(menu_folder,sub_menu)\n",
    "                    if not os.path.exists(sub_menu_folder):\n",
    "                        os.makedirs(sub_menu_folder)\n",
    "                print(\"Folder Hierarchy Created\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred : {e}\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEBI DATA SCRAPER\n",
    "<blockquote>\n",
    "class SEBIDataScraper:\n",
    "    '''\n",
    "    SEBIDataScraper is a class designed for scraping data from the Securities and Exchange Board of India (SEBI) website. \n",
    "    It provides functionality to navigate through SEBI's web pages, collect links to HTML and PDF files, and download \n",
    "    these files for further analysis or processing.\n",
    "\n",
    "    Attributes:\n",
    "        data: A list to store the scraped data.\n",
    "    \n",
    "    Methods:\n",
    "        __init__: Initializes the SEBIDataScraper object and creates a CSV file if it doesn't already exist to store\n",
    "            links to PDF files.\n",
    "        navigate_pagination_and_collect_links: Navigates through paginated web pages, extracts links to HTML and PDF \n",
    "            files from tables, and stores them in the data attribute.\n",
    "        collect_html_links: Collects HTML links for a given menu and submenu combination.\n",
    "        collect_pdf_links_for_all_for_all_for_all: Collects PDF links for a given menu and submenu combination, also extracts PDF text if available.\n",
    "        download_pdf: Downloads a PDF file from a given URL to the specified download path.\n",
    "        download_html: Downloads an HTML file from a given URL to the specified download path.\n",
    "        count_files: Counts the number of files of a specific type (PDF or HTML) in a given menu and submenu folder.\n",
    "        create_list_of_links: Creates a list of PDF and HTML links for a given menu and submenu.\n",
    "        download_files: Downloads PDF and HTML files concurrently for a given menu and submenu.\n",
    "    '''\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "#|eval:True \n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import logging\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import concurrent.futures\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import hashlib\n",
    "import os\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "           \n",
    "class SEBIDataScraper:\n",
    "    logger_data_scraper = setup_logger(\"sebi_data_scraper.log\")\n",
    "    def __init__(self):\n",
    "        self.db_handler_obj = DBHandler()\n",
    "        \n",
    "        # OLD => Create CSV files with menus, if they dont already exists\n",
    "        # NEW => Create sqlitedb (if it doesn't exists) to store the the extracted text \n",
    "        # if(not os.path.exists(pdf_links_of_all)):\n",
    "        #     df = pd.DataFrame(columns=columns_for_pdf_links_of_all)\n",
    "        #     df.to_csv(pdf_links_of_all, index=True)\n",
    "        #     print(f\"CSV file {pdf_links_of_all} with column names created successfully.\")\n",
    "        self.data = []\n",
    "        print(\"SEBI Data Scrapper Object is created\")\n",
    "    \n",
    "    def navigate_pagination_and_collect_links(self,url,type,sub_type):\n",
    "        \n",
    "        # logging.basicConfig(filename='selenium_next_button.log', level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "        # logging.info(f'Entering navigate_pagination_and_collect_links method, {url}')\n",
    "        type = type.lower()\n",
    "        sub_type = sub_type.lower()\n",
    "        \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        page_num = 0\n",
    "\n",
    "        while(1):\n",
    "           \n",
    "            print(\"inside while\")\n",
    "            try:\n",
    "\n",
    "                time.sleep(1)\n",
    "                ########### GEL HTML/PDF LINKS FROM THE TABLE ###########\n",
    "                html_content = driver.page_source\n",
    "                print(f\"html_content of {page_num} received \")\n",
    "                soup = BeautifulSoup(html_content,'html.parser')\n",
    "                table = soup.find('table')\n",
    "                if(table == None):\n",
    "                    return\n",
    "                no_of_rows = len(table.find_all('tr'))\n",
    "                print(f\"Page : {page_num} | No of rows: {no_of_rows}\")\n",
    "\n",
    "                # The html contains table which hold info - PDF Title, Data, PDF Viewer Link\n",
    "                for row in table.find_all('tr'):\n",
    "                    if(row == None):\n",
    "                        continue\n",
    "                    all_cells = row.find_all('td')\n",
    "                    #Exclude the first row\n",
    "                    if(len(all_cells) == 0):\n",
    "                        continue\n",
    "\n",
    "                    anchor_tag = all_cells[1].find('a')\n",
    "                    date = all_cells[0].text\n",
    "                    title = all_cells[1].text\n",
    "                    href_link = anchor_tag.get('href')\n",
    "                    new_row = {\n",
    "                        \"title\" : title,\n",
    "                        \"date\" : date,\n",
    "                        \"html_link\" : href_link,\n",
    "                        \"pdf_link\" : \"\",\n",
    "                        \"type\": type,\n",
    "                        \"sub_type\" : sub_type,\n",
    "                        \"file_name\" : \"\",\n",
    "                        \"file_type\" : \"\",\n",
    "                        \"pdf_text\" : \"\"\n",
    "                    }\n",
    "                    \n",
    "                    self.data.append(new_row)\n",
    "                    print(\"New Row appended to data :\", new_row)\n",
    "\n",
    "                ############ ENDS HERE ############\n",
    "\n",
    "                time.sleep(2)\n",
    "                try:\n",
    "                    # Check if the page contains pagination or is it a single page\n",
    "                    if not (driver.find_element(By.CLASS_NAME, \"pagination_outer\")):\n",
    "                        return\n",
    "\n",
    "                    WebDriverWait(driver, 20).until(\n",
    "                        EC.element_to_be_clickable((By.CLASS_NAME, \"pagination_outer\"))\n",
    "                        # Check if the pagination_outer is loaded, so we can move on to next step\n",
    "                    )\n",
    "\n",
    "                    # if we cant find the next_button, it means we are at the last page\n",
    "                    if not (driver.find_elements(By.XPATH, \"//*[@title='Next']\")):\n",
    "                        return\n",
    "\n",
    "                    next_button = driver.find_element(By.XPATH, \"//*[@title='Next']\")\n",
    "                    time.sleep(2)\n",
    "\n",
    "                    next_button.click()\n",
    "                    # logging.info('Clicked on Next Button')\n",
    "                except NoSuchElementException as e:\n",
    "                    print(e)\n",
    "                    print(\".pagination_outer is not present and/or next button not present\")\n",
    "                    return\n",
    "\n",
    "                driver.implicitly_wait(5)\n",
    "                # logging.info('Waited for download to complete')\n",
    "            except Exception as e:\n",
    "                print(\"EXCEPTION occurred :\",e)\n",
    "                # logging.error(f'An exception occurred: {str(e)}')\n",
    "                return\n",
    "            finally:\n",
    "                #driver.quit()\n",
    "                page_num += 1\n",
    "                print(\"Exiting navigation_pagination_and_collect_links\")\n",
    "                # logging.info('Exiting navigation_pagination_and_collect_links')\n",
    "\n",
    "    # Function that collects html links from navigating to each menu links\n",
    "    def collect_html_links(self,menu_to_scrape, submenu_to_scrape):\n",
    "        print(f\"Entering collect_html_links for [{menu_to_scrape}] | [{submenu_to_scrape}]\")\n",
    "        df = pd.read_csv(urls_of_sebi_menu_csv_path)\n",
    "        \n",
    "        for _,row in df.iterrows():\n",
    "            menu = row['menu']\n",
    "            submenu = row['submenu']\n",
    "            url = row['url']\n",
    "            # print(f\"Creating list of urls using menu: {menu} and sub_menu: {submenu}\")\n",
    "            menu = menu.replace(\" \",\"_\")\n",
    "            submenu = submenu.replace(\" \",\"_\")\n",
    "            if(menu.lower() == menu_to_scrape.lower() and submenu.lower() == submenu_to_scrape.lower()):\n",
    "                print(f\"Accessing the link {url}\")\n",
    "                self.navigate_pagination_and_collect_links(url,menu,submenu)\n",
    "            # print(f\"Row added in CSV: {menu} | sub_menu: {submenu}\")\n",
    "            \n",
    "            \n",
    "    def soup_returner(self,url):\n",
    "        soup = BeautifulSoup()\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            retry = HTTPAdapter(max_retries=5)\n",
    "            session.mount(\"http://\", retry)\n",
    "            session.mount(\"https://\", retry)\n",
    "            read = session.get(url,verify=False)\n",
    "            html_content = read.text\n",
    "            print(html_content)\n",
    "            soup = BeautifulSoup(html_content,'html.parser')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"URL \", url)\n",
    "            print(\"Exception occured : \",e )\n",
    "            \n",
    "        print(\"soup returned.\")\n",
    "        return soup\n",
    "    \n",
    "    def collect_pdf_link(self,html_link):\n",
    "        url = html_link\n",
    "        print(f\"Collecting pdf link for {url}\")\n",
    "        soup = self.soup_returner(url)\n",
    "        only_anchor_tags = soup.find_all('iframe')\n",
    "        print(\"only_anchor_tags : \", only_anchor_tags)\n",
    "        new_pdf_link = \"\"\n",
    "        for link in only_anchor_tags:\n",
    "            print(\"inside for loop\")\n",
    "            href_link = link.get('src')\n",
    "            print(\"href link has been fetched : \",href_link)\n",
    "            if (href_link!=None) and href_link.lower().endswith(\".pdf\"):\n",
    "                pdf_link = href_link\n",
    "                new_pdf_link = urljoin(home_page_url,pdf_link)\n",
    "        return new_pdf_link\n",
    "        \n",
    "    \n",
    "    def collect_pdf_links_for_all(self,menu_to_scrape, submenu_to_scrape):\n",
    "        print(\"inside collect_pdf_links_for_all\")\n",
    "        self.collect_html_links(menu_to_scrape, submenu_to_scrape)\n",
    "        print(f\"collecting pdf links for : [{menu_to_scrape}] | [{submenu_to_scrape}]\")\n",
    "\n",
    "        for row in self.data:\n",
    "            url = row['html_link']\n",
    "            new_pdf_link = self.collect_pdf_link(url)\n",
    "            # print(f\"Collecting pdf link for {url}\")\n",
    "            # soup = self.soup_returner(url)\n",
    "            # only_anchor_tags = soup.find_all('iframe')\n",
    "            # new_pdf_link = \"\"\n",
    "            # for link in only_anchor_tags:\n",
    "            #     href_link = link.get('src')\n",
    "            #     if href_link!=None and href_link.lower().endswith(\".pdf\"):\n",
    "            #         pdf_link = href_link\n",
    "            #         new_pdf_link = urljoin(home_page_url,pdf_link)\n",
    "            row['pdf_link'] = new_pdf_link\n",
    "\n",
    "            #storing the name of the pdf for the future use\n",
    "            split_pdf_link = new_pdf_link.split(\"/\")\n",
    "            row['file_name'] = split_pdf_link[-1]\n",
    "            row['file_type'] = \"pdf\"\n",
    "            \n",
    "            print(\"Pdf link fetched : \", new_pdf_link)\n",
    "            # If the pdf name is blank that means the content is html file\n",
    "            \n",
    "            if (new_pdf_link == \"\"):\n",
    "                url_base64 = base64.b64encode(url.encode()).decode()\n",
    "                # row['file_name'] = url_base64+\".html\"\n",
    "                filename_hashed = hashlib.sha256(url_base64.encode('utf-8')).hexdigest()\n",
    "                row['file_name'] = filename_hashed+\".html\"\n",
    "                row['file_type'] = \"html\"\n",
    "\n",
    "        # Saving the data into sqlite db\n",
    "        for row in self.data:\n",
    "            print(f\"saving row in db : {row}\")\n",
    "            self.db_handler_obj.insert_data(row)\n",
    "        # df2 = pd.DataFrame(self.data)\n",
    "        # df2.to_csv(pdf_links_of_all, mode='a', index=True)\n",
    "\n",
    "\n",
    "    def download_pdf(self,url,download_path,file_name):\n",
    "        logging.basicConfig(filename='selenium_log.log', level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')\n",
    "        logging.info(f\"Downloading {file_name} in {download_path}\")\n",
    "        print(f\"Downloading {file_name} in {download_path}\")\n",
    "\n",
    "        if(not url.startswith(\"http\")):\n",
    "            logging.info(\"Not a valid url\")\n",
    "            return\n",
    "        \n",
    "        pdf_path = os.path.join(download_path,file_name)\n",
    "        if(os.path.exists(pdf_path)):\n",
    "            print(f\"The pdf {file_name} already exists\")\n",
    "            logging.info(f\"The pdf {file_name} already exists\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            logging.info(f'Status of url : {url} is {response.status_code}')\n",
    "            if(response.status_code != 200):\n",
    "                print(f\"Status of {url} is {response.status_code}\")\n",
    "            options = webdriver.ChromeOptions()\n",
    "\n",
    "            prefs = {\n",
    "                \"download.default_directory\": download_path,\n",
    "                'download.prompt_for_download': False,\n",
    "                'plugins.always_open_pdf_externally': True\n",
    "            }\n",
    "\n",
    "            options.add_argument(\"--headless=new\")\n",
    "\n",
    "            options.add_experimental_option('prefs',prefs)\n",
    "\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(url)\n",
    "            logging.info(f'Navigated to URL: {url}')\n",
    "            # Wait for some time to ensure the PDF is loaded\n",
    "            #driver.implicitly_wait(10)\n",
    "\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.ID, \"download\"))\n",
    "            )\n",
    "\n",
    "            #Find the DOWNLOAD Button from the pdfviewer using the id of button tag\n",
    "            download_button = driver.find_element(By.ID, \"download\")\n",
    "            time.sleep(3)\n",
    "            download_button.click()\n",
    "            logging.info('Clicked on PDF download Button')\n",
    "            driver.implicitly_wait(10)\n",
    "            logging.info('Waited for download to complete')\n",
    "            time.sleep(6)\n",
    "\n",
    "            # num = rename_most_recent_pdf(download_path,file_name,past_num_of_files_present)\n",
    "            # past_num_of_files_present = num\n",
    "            # print(f\"Past num of files present : {past_num_of_files_present}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"EXCEPTION \",e)\n",
    "            logging.error(f'An error occurred: {str(e)}')\n",
    "        finally:\n",
    "            #driver.quit()\n",
    "            logging.info('Browser session closed')\n",
    "    \n",
    "    def download_pdf_new(self,row):\n",
    "        url = row['pdf_link']\n",
    "        html_url = row['html_link']\n",
    "        title = row['title']\n",
    "        type = row['type']\n",
    "        sub_type = row['sub_type']\n",
    "        file_name = row['file_name']\n",
    "        \n",
    "        type = type.replace(\" \",\"_\")\n",
    "        sub_type = sub_type.replace(\" \",\"_\")\n",
    "        # SEBI_data_extraction_base_folder = r\"D:\\Educational\\Sarvam AI\\Python\\SEBI\\SEBI_Extracted_Data\"\n",
    "        type_folder_path = os.path.join(SEBI_data_extraction_base_folder,type)\n",
    "        sub_type_folder_path = os.path.join(type_folder_path,sub_type)\n",
    "        download_path = sub_type_folder_path\n",
    "        \n",
    "        print(\"URL Accessing \",url)\n",
    "        \n",
    "        file_path = os.path.join(download_path, file_name)\n",
    "        if(os.path.exists(file_path)):\n",
    "            print(f\"the file {file_path} already exists.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            # logging.info(f'Status of url : {url} is {response.status_code}')\n",
    "            if(response.status_code != 200):\n",
    "                print(f\"Status of {url} is {response.status_code}\")\n",
    "            options = webdriver.ChromeOptions()\n",
    "\n",
    "            prefs = {\n",
    "                \"download.default_directory\": download_path,\n",
    "                'download.prompt_for_download': False,\n",
    "                'plugins.always_open_pdf_externally': True\n",
    "            }\n",
    "\n",
    "            options.add_argument(\"--headless=new\")\n",
    "\n",
    "            options.add_experimental_option('prefs',prefs)\n",
    "\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(url)\n",
    " \n",
    "\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.ID, \"download\"))\n",
    "            )\n",
    "\n",
    "            #Find the DOWNLOAD Button from the pdfviewer using the id of button tag\n",
    "            download_button = driver.find_element(By.ID, \"download\")\n",
    "            time.sleep(2)\n",
    "            download_button.click()\n",
    "            \n",
    "            driver.implicitly_wait(10)\n",
    "            # logging.info('Waited for download to complete')\n",
    "            time.sleep(6)\n",
    "            \n",
    "            timeout = 25  # Maximum wait time in seconds\n",
    "            start_time = time.time()\n",
    "            while time.time() - start_time < timeout:\n",
    "                print(str(time.time() - start_time))\n",
    "                if os.path.isfile(os.path.join(download_path, file_name)):\n",
    "                    print(f\"Download complete! for {file_name} in {download_path}\")\n",
    "                    break\n",
    "                time.sleep(1)  # Check every 1 second\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Download button not found for URL: {url}\")\n",
    "        finally:\n",
    "            # driver.quit()\n",
    "            print(\"Exiting download_pdf_new\")\n",
    "    \n",
    "    # @staticmethod\n",
    "    def download_html(self,url,download_path, file_name):\n",
    "        print(f\"Downloading {file_name} in {download_path}\")\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # filename_hashed = hashlib.sha256(url.encode('utf-8')).hexdigest()\n",
    "\n",
    "        file_download_path = os.path.join(download_path,file_name)\n",
    "        if(os.path.exists(file_download_path)):\n",
    "            print(f\"The file {file_name} already exists\")\n",
    "            logging.info(f\"The file {file_name} already exists\")\n",
    "            return\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(response.status_code)\n",
    "            with open(file_download_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"{url} is downloaded into {download_path}\")\n",
    "        else:\n",
    "            print(\"Failed to download HTML:\", response.status_code)\n",
    "            \n",
    "    def download_html_new(self,row):\n",
    "        # url = row['pdf_link']\n",
    "        html_url = row['html_link']\n",
    "        title = row['title']\n",
    "        type = row['type']\n",
    "        sub_type = row['sub_type']\n",
    "        file_name = row['file_name']\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f\" File : {file_name} and LENGTH : {len(file_name)}\",file_name)\n",
    "        \n",
    "        \n",
    "        type = type.replace(\" \",\"_\")\n",
    "        sub_type = sub_type.replace(\" \",\"_\")\n",
    "        type_folder_path = os.path.join(SEBI_data_extraction_base_folder,type)\n",
    "        sub_type_folder_path = os.path.join(type_folder_path,sub_type)\n",
    "        download_path = sub_type_folder_path\n",
    "        \n",
    "        file_path = os.path.join(download_path, file_name)\n",
    "        if(os.path.exists(file_path)):\n",
    "            print(f\"the file {file_path} already exists.\")\n",
    "            return\n",
    "        \n",
    "        response = requests.get(html_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(response.status_code)\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"{html_url} is downloaded into {download_path}\")\n",
    "        else:\n",
    "            print(\"Failed to download HTML:\", response.status_code)\n",
    "    \n",
    "    \n",
    "    # def download_all_files(self):\n",
    "    #     print(\"Downloading in...\",SEBI_data_extraction_base_folder)\n",
    "    #     for row in self.data:\n",
    "\n",
    "    #         pdf_url = row['pdf_link']\n",
    "    #         html_url = row['html_link']\n",
    "    #         title = row['title']\n",
    "    #         sub_type = row['type']\n",
    "    #         sub_sub_type = row['sub_type']\n",
    "    #         file_name = row['file_name']\n",
    "\n",
    "    #         sub_type = sub_type.replace(\" \",\"_\")\n",
    "    #         sub_sub_type = sub_sub_type.replace(\" \",\"_\")\n",
    "            \n",
    "    #         sub_type_folder_path = os.path.join(SEBI_data_extraction_base_folder,sub_type)\n",
    "    #         sub_sub_type_folder_path = os.path.join(sub_type_folder_path,sub_sub_type)\n",
    "    #         download_path = sub_sub_type_folder_path\n",
    "\n",
    "    #         if (pdf_url == \"\"):\n",
    "    #             self.download_html(html_url, download_path, file_name)\n",
    "    #             continue\n",
    "    #         self.download_pdf(pdf_url, download_path,file_name)\n",
    "    \n",
    "    def count_files(self, menu,submenu,file_type):\n",
    "    # Ensure the folder exists\n",
    "\n",
    "        folder_path = os.path.join(SEBI_data_extraction_base_folder,menu)\n",
    "        folder_path = os.path.join(folder_path,submenu)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"The folder '{folder_path}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        # Initialize the count\n",
    "        file_count = 0\n",
    "        total_files = 0\n",
    "        # Iterate through all items in the folder\n",
    "        for item in os.listdir(folder_path):\n",
    "            # Check if the item is a file\n",
    "            if os.path.isfile(os.path.join(folder_path, item)):\n",
    "                # If it's a file, increment the count\n",
    "                total_files += 1\n",
    "                if(item.lower().endswith(file_type)):\n",
    "                    file_count += 1\n",
    "\n",
    "        return file_count,total_files\n",
    "    \n",
    "    def create_list_of_links(self,menu,submenu):\n",
    "\n",
    "        df = pd.read_csv(pdf_links_of_all)\n",
    "        pdf_link_list = []\n",
    "        html_link_list = []\n",
    "        \n",
    "        print(f\"Create list of links {menu} and {submenu}\")\n",
    "                \n",
    "        # Fetch the rows from the table having type==menu and sub_type==submenu\n",
    "        fetched_result = []\n",
    "        # What will be returned here?\n",
    "        fetched_result = self.db_handler_obj.fetch_rows_by_type_and_subtype(menu.lower(),submenu.lower())\n",
    "        \n",
    "        for row in fetched_result:\n",
    "            print(\"Fetched Result Row:\")\n",
    "            print(row)\n",
    "            html_link,title,date,pdf_link,type,sub_type,file_name,file_type,pdf_text = row\n",
    "            \n",
    "            def create_download_path(type, sub_type):\n",
    "                type = type.replace(\" \",\"_\")\n",
    "                sub_type = sub_type.replace(\" \",\"_\")\n",
    "                \n",
    "                type_folder_path = os.path.join(SEBI_data_extraction_base_folder,type)\n",
    "                sub_type_folder_path = os.path.join(type_folder_path,sub_type)\n",
    "                download_path = sub_type_folder_path\n",
    "                return download_path\n",
    "            \n",
    "            download_path = create_download_path(type,sub_type)\n",
    "            file_path = os.path.join(download_path,file_name)\n",
    "            \n",
    "            if(pdf_link == \"\" or pdf_link == None):\n",
    "                # It means the file is html (NOT PDF)\n",
    "                if(os.path.exists(file_path)):\n",
    "                    print(f\"Not adding to list, becoz The file {file_path} already exists\")\n",
    "                    continue\n",
    "                pdf_link_list.append(row)\n",
    "            else:\n",
    "                if(os.path.exists(file_path)):\n",
    "                    print(f\"Not adding to list, becoz The file {file_path} already exists\")\n",
    "                    continue\n",
    "                html_link_list.append(row)\n",
    "                \n",
    "        return pdf_link_list,html_link_list\n",
    "\n",
    "    \n",
    "    # def download_files(self, menu, sub_menu):\n",
    "    #     df = pd.read_csv(pdf_links_of_all)\n",
    "        \n",
    "    #     for _,row in df.iterrows():\n",
    "    #         pdf_url = row['pdf_link']\n",
    "    #         html_url = row['html_link']\n",
    "    #         title = row['title']\n",
    "    #         type = row['type']\n",
    "    #         sub_type = row['sub_type']\n",
    "    #         file_name = row['file_name']\n",
    "    #         flag = False\n",
    "    #         if(pd.isna(row['pdf_link'])):\n",
    "    #             flag = True\n",
    "    #         if(pd.isna(html_url) or pd.isna(title) or pd.isna(type) or pd.isna(sub_type) or pd.isna(file_name)):\n",
    "    #             print(\"Values are empty.\")\n",
    "    #             continue\n",
    "            \n",
    "    #         type = type.replace(\" \",\"_\")\n",
    "    #         sub_type = sub_type.replace(\" \",\"_\")\n",
    "            \n",
    "    #         type_folder_path = os.path.join(SEBI_data_extraction_base_folder,type)\n",
    "    #         sub_type_folder_path = os.path.join(type_folder_path,sub_type)\n",
    "    #         download_path = sub_type_folder_path\n",
    "            \n",
    "    #         if (sub_menu == None):\n",
    "    #             if(menu == type.lower()):\n",
    "    #                 if (flag):\n",
    "    #                     file_name = row['file_name']\n",
    "    #                     filename_hashed = hashlib.sha256(html_url.encode('utf-8')).hexdigest()\n",
    "    #                     row['file_name'] = filename_hashed\n",
    "    #                     self.download_html(html_url, download_path, filename_hashed)\n",
    "    #                 else:\n",
    "    #                     self.download_pdf(pdf_url, download_path,file_name)\n",
    "    #         else:\n",
    "    #             if(menu == type.lower() and sub_menu == sub_type.lower()):\n",
    "    #                 if (flag):\n",
    "    #                     file_name = row['file_name']\n",
    "    #                     filename_hashed = hashlib.sha256(html_url.encode('utf-8')).hexdigest()\n",
    "    #                     row['file_name'] = filename_hashed\n",
    "    #                     self.download_html(html_url, download_path, filename_hashed)\n",
    "    #                 else:\n",
    "    #                     self.download_pdf(pdf_url, download_path,file_name)\n",
    "                        \n",
    "    # def download_files2(self, row, menu, sub_menu):\n",
    "        \n",
    "    #     pdf_url = row['pdf_link']\n",
    "    #     html_url = row['html_link']\n",
    "    #     title = row['title']\n",
    "    #     type = row['type']\n",
    "    #     sub_type = row['sub_type']\n",
    "    #     file_name = row['file_name']\n",
    "        \n",
    "    #     print(f\"Downloading..... {html_url}\")\n",
    "        \n",
    "    #     type = type.replace(\" \",\"_\")\n",
    "    #     sub_type = sub_type.replace(\" \",\"_\")\n",
    "        \n",
    "    #     type_folder_path = os.path.join(SEBI_data_extraction_base_folder, type)\n",
    "    #     sub_type_folder_path = os.path.join(type_folder_path, sub_type)\n",
    "    #     download_path = sub_type_folder_path\n",
    "\n",
    "    #     if not os.path.exists(download_path):\n",
    "    #         os.makedirs(download_path)\n",
    "            \n",
    "    #     if (sub_menu == None):\n",
    "    #         if(menu == type.lower()):\n",
    "    #             if (pdf_url == \"\"):\n",
    "    #                 self.download_html(html_url, download_path, file_name)\n",
    "    #             else:\n",
    "    #                 self.download_pdf(pdf_url, download_path,file_name)\n",
    "    #         else:\n",
    "    #             if(menu == type.lower() and sub_menu == sub_type.lower()):\n",
    "    #                 if (pdf_url == \"\"):\n",
    "    #                     self.download_html(html_url, download_path, file_name)\n",
    "    #                 else:\n",
    "    #                     self.download_pdf(pdf_url, download_path,file_name)\n",
    "                        \n",
    "    def download_files3(self,menu,submenu):\n",
    "        print(\"inside download_files3\")\n",
    "        print(menu,submenu)\n",
    "        pdf_urls = []\n",
    "        html_urls = []\n",
    "        pdf_urls,html_urls = self.create_list_of_links(menu,submenu)\n",
    "        # pdf_urls,html_urls,total_pdf_count,total_html_count = self.create_list_of_links(menu,submenu)\n",
    "        for row in pdf_urls:\n",
    "            print(row)\n",
    "            print(row['pdf_link'])\n",
    "            print(\"-------------------\")\n",
    "        print(\"HTML Links ::: \")\n",
    "        for row in html_urls:\n",
    "            print(row['html_link'])\n",
    "            print(\"-------------------\") \n",
    "        \n",
    "        print(\"List has been created, moving on to downloads\")\n",
    "        \n",
    "        # Concurrently download PDFs\n",
    "        round = 1\n",
    "        file_count,total_files_downloaded = self.count_files(menu,submenu,\"pdf\")\n",
    "        i = 0\n",
    "        \n",
    "        # for row in pdf_urls:\n",
    "        #     self.download_pdf_new(row)\n",
    "        #     print(f\"Download completed : {i}/{len(pdf_urls)} \")\n",
    "        #     i += 1\n",
    "        while(1):\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                executor.map(self.download_pdf_new, pdf_urls)\n",
    "            pdf_urls = []\n",
    "            html_urls = []\n",
    "            pdf_urls,html_urls,total_pdf_count,total_html_count = self.create_list_of_links(menu,submenu)\n",
    "            \n",
    "        ##########################      \n",
    "        # while(total_files_downloaded < total_pdf_count):\n",
    "        #     print(f\"Round : {round}\")\n",
    "        #     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        #         executor.map(self.download_pdf_new, pdf_urls)\n",
    "        #     # for row in pdf_urls:\n",
    "        #     #     self.download_pdf_new(row)\n",
    "        #     file_count,_ = self.count_files(menu,submenu,\"pdf\")\n",
    "        #     print(f\"File Count : {file_count}/{total_files_downloaded}\")\n",
    "        #     round += 1\n",
    "        ####################\n",
    "        \n",
    "        # Concurrently download HTMLs\n",
    "        round = 1\n",
    "        i = 0\n",
    "        file_count,total_files_downloaded = self.count_files(menu,submenu,\"html\")\n",
    "        for row in html_urls:\n",
    "            self.download_html_new(row)\n",
    "            print(f\"Download completed : {i}/{len(html_urls)} \")\n",
    "            i += 1\n",
    "        # while(total_files_downloaded < total_html_count):\n",
    "        #     print(f\"Round : {round}\")\n",
    "        #     # with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        #     #     executor.map(self.download_html_new, html_urls)\n",
    "        #     for row in html_urls:\n",
    "        #         self.download_html_new(row)\n",
    "        #     file_count,_ = self.count_files(menu,submenu,\"html\")\n",
    "        #     print(f\"File Count : {file_count}/{total_files_downloaded}\")\n",
    "        #     round += 1\n",
    "        ###########################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # def download_files_concurrently(self, menu, sub_menu):\n",
    "    #     pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())  # Use number of available CPUs\n",
    "    #     func = lambda row: self.download_file(row, menu, sub_menu)\n",
    "    #     df = pd.read_csv(pdf_links_of_all)\n",
    "    #     pool.map(func, df.iterrows())\n",
    "    #     pool.close()\n",
    "    #     pool.join()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBHandler - Methods to handle Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "#|eval:True\n",
    "import sqlite3\n",
    "\n",
    "class DBHandler():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.create_db()\n",
    "        self.create_tables()\n",
    "        pass\n",
    "    \n",
    "    def create_db(self):\n",
    "        if not os.path.exists(DB_NAME):\n",
    "            print(\"Database does not exist. Creating it....\")\n",
    "            open(DB_NAME, 'w').close()\n",
    "            # Create Tables under it  \n",
    "            self.create_tables()\n",
    "        else:\n",
    "            print(f\"database {DB_NAME} already exists.\")\n",
    "            \n",
    "    def create_tables(self):\n",
    "        print(f\"Creating table [{TABLE_SEBI_RECORDS}] in database :[{DB_NAME}]\")\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(DB_NAME)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            query1 = f'''CREATE TABLE IF NOT EXISTS {TABLE_SEBI_RECORDS} (\n",
    "                            html_link TEXT PRIMARY KEY,\n",
    "                            title TEXT,\n",
    "                            date TEXT,\n",
    "                            pdf_link TEXT,\n",
    "                            type TEXT,\n",
    "                            sub_type TEXT,\n",
    "                            file_name TEXT,\n",
    "                            file_type TEXT,\n",
    "                            pdf_text TEXT\n",
    "                        )'''\n",
    "            \n",
    "            cursor.execute(query1)\n",
    "            \n",
    "            conn.commit()\n",
    "            print(\"tables successfully created!\")\n",
    "        except Exception as e:\n",
    "            print(\"Exception occured while creating table : \",e)\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                \n",
    "    def insert_data(self,row):\n",
    "        print(f\"Inserting values into {DB_NAME} and table {TABLE_SEBI_RECORDS}\")\n",
    "\n",
    "        html_link = row['html_link']\n",
    "        title = row['title']\n",
    "        date = row['date']\n",
    "        pdf_link = row['pdf_link']\n",
    "        type = row['type']\n",
    "        sub_type = row['sub_type']\n",
    "        file_name = row['file_name']\n",
    "        file_type = row['file_type']\n",
    "        pdf_text = row['pdf_text']\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(DB_NAME)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Insert data into the database\n",
    "            query = f'''INSERT INTO TABLE_SEBI_RECORDS (html_link, title, date, pdf_link, \n",
    "            type, sub_type, file_name, file_type, pdf_text) \n",
    "            VALUES (?,?,?,?,?,?,?,?,?)'''\n",
    "\n",
    "            cursor.execute(query, (html_link,title,date,pdf_link,type,sub_type,file_name,file_type,pdf_text))\n",
    "\n",
    "            # Commit changes and close connection\n",
    "            conn.commit()\n",
    "            print(f\"Data Scored Successfully in {TABLE_SEBI_RECORDS}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occured while inserting a row : {row}\")\n",
    "            print(e)\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                \n",
    "    def update_data(row, pdf_text):\n",
    "        html_link = row['html_link']\n",
    "        print(f\"updating a pdf_text for {html_link}\")\n",
    "        try:            \n",
    "            conn = sqlite3.connect(DB_NAME)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Update the pdf_text for the given html_link\n",
    "            cursor.execute(\"UPDATE TABLE_SEBI_RECORDS SET pdf_text = ? WHERE html_link = ?\", (pdf_text, html_link))\n",
    "            \n",
    "            # Commit the transaction\n",
    "            conn.commit()\n",
    "            print(\"PDF text updated successfully.\")\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"Exception occured for {html_link}: \", e)\n",
    "        finally:\n",
    "            # Close the database connection\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                \n",
    "                \n",
    "    def fetch_rows_by_type_and_subtype(type,sub_type):\n",
    "        try:\n",
    "            # Connect to the SQLite database\n",
    "            conn = sqlite3.connect(DB_NAME)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Execute the SQL query to fetch rows based on the condition\n",
    "            cursor.execute(f\"SELECT * FROM {TABLE_SEBI_RECORDS} WHERE type = ? AND sub_type = ?\", (type,sub_type))\n",
    "            \n",
    "            # Fetch all rows that satisfy the condition\n",
    "            rows = cursor.fetchall()\n",
    "            return rows\n",
    "\n",
    "        except sqlite3.Error as e:\n",
    "            print(\"Error fetching rows:\", e)\n",
    "            return None\n",
    "        finally:\n",
    "            # Close the database connection\n",
    "            if conn:\n",
    "                conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: True\n",
    "#|eval: True\n",
    "menu_collectors = SEBIMenuCollector()\n",
    "scraper = SEBIDataScraper()\n",
    "menu_collectors.collect_menu_links()\n",
    "menu_collectors.create_folder_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: False\n",
    "#|eval: False\n",
    "#|hide\n",
    "new_pdf_link = scraper.collect_pdf_link(\"https://www.sebi.gov.in/legal/rules/mar-2021/sebi-annual-report-rules-2021_49611.html\")\n",
    "print(new_pdf_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: False\n",
    "#|eval: False\n",
    "#|hide\n",
    "scraper.collect_pdf_links_for_all(\"legal\", \"rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    # Traverse a sql table row by row and whenever pdf_text is NULL, go to the download directory => check if the file present, if yes => extract the pdf_text using ocr\n",
    "    # The below function will just return the pdf_text by accessing the file_path\n",
    "    # check if the file_path exists or not\n",
    "    def extract_data_from_pdf(file_path):\n",
    "        pdf_text = \"\"\n",
    "        if(not os.path.exists(file_path)):\n",
    "            print(f\"The {file_path} doesnt exists!\")\n",
    "            return pdf_text\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sebi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
